# attack-and-defense
2020.08.07,我看完了李宏毅老师的神经网络攻击与防御篇章的课程后，感悟颇深。由于神经网络的参数量较大，这导致了我们很难地理解每一个参数的意义，也导致了神经网络的可解释性差。因此，人们通常将其称为黑盒。为了打开这一个潘多拉魔盒，目前已有许多的学者投身于解决神经网络解释性差的问题。近些年来，学者们也已经提出了许多的方法来可视化神经网络做出决策的依据，如CAM和GRAD-CAM。这两种方法是通过卷积层的feature map来构建图像的heatmap，使得人们能够直觉地观看到卷积神经网络所看到的“图像区域”。这也推动了神经网络能够走出实验室，真正地应用到现实生活之中。但是在实际情况之中，神经网络所面临的输入环境是复杂多样，它不仅会收到各种各样的输入格式，如PNG,JPG等等。除此之外，它也可能收到包含各种各样的内容的图像（如不包含识别object的图像）。对于神经网络，无论收到何种内容的图像，它最终都会给出一个probability distribution。所以，当我们给其一张不包含其分类object的图像时，它一定会将其归类为其中一类。这显然是不合理。对于这一个问题，我们可以通过引入的一个other class来解决。除此之外，神经网咯采用分层机制逐步地从pixel space来提取图像的语义信息。当图像被加入了肉眼无法区分的噪声时，这可能导致模型分类错误。所以，神经网络既是intellegence,也是fool。
# Experiment
1.证明对于给定的object的图像和不包含此obejct的图像，神经网络可能会将其分类为第一类。  
在这里，我使用最大激活值的方法来最大化在imagenet上训练的vgg16网络的某一label概率以获得其输入图像。代码见generate.py。迭代过程中，输入图像的LOSS和其对应label的概率如下图所示：  
![image]()
![image]()
对于卷积神经网络能够将不包含给定object的图像也分为了object class的解释如下：训练图像的space为现实生活的一个subsapce,我们将模型在这一个subspace所学习到的分类界面放入到一个更大的space中，虽然它能够很好地划分subspace中的object,但是它却缺少了subspace中的objects与那个更大的space中其它object的分界线。所以这导致了模型在一个更大的space中，它也能够将不包含给定Object对象的图像也为object类。这也是在可视化模型结果的时候，我们需要引入图像领域的prior knowledge的原因。
